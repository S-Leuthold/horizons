#' CARS-Based Feature Selection for Spectral Predictors
#'
#' Applies the Competitive Adaptive Reweighted Sampling (CARS) algorithm to select
#' the minimal optimal set of spectral predictors through iterative PLS regression
#' with exponentially decreasing retention rates and adaptive reweighting.
#'
#' @param recipe A `recipes::recipe()` object.
#' @param ... Selector functions to choose spectral columns (e.g., `all_predictors()`).
#' @param outcome Character. Name of the outcome variable to use for model fitting.
#' @param role Character. Role for retained variables. Default is `"predictor"`.
#' @param trained Logical. Required by `recipes`; indicates if the step has been prepped.
#' @param skip Logical. Should this step be skipped during `bake()`? Default is `FALSE`.
#' @param id Character. Unique identifier for the step. Auto-generated by default.
#'
#' @return A `step_select_cars` object that can be added to a recipe.
#'
#' @details
#' This step uses the CARS feature selection algorithm, which iteratively performs
#' PLS regression with Monte Carlo sampling and exponentially decreasing retention
#' rates to identify the minimal optimal wavelength set. The algorithm is run at
#' `prep()` time.
#'
#' For best results, this step should be applied *after* spectral preprocessing (e.g. SNV,
#' derivatives) and *before* PCA or model fitting. CARS is particularly effective for
#' identifying minimal wavelength sets that maintain predictive performance.
#'
#' @export

## -----------------------------------------------------------------------------
## Step 1: User-facing step constructor
## -----------------------------------------------------------------------------

#' @export
step_select_cars <- function(recipe,
                             ...,
                             outcome,
                             role    = "predictor",
                             trained = FALSE,
                             skip    = FALSE,
                             id      = recipes::rand_id("select_cars")) {

  terms <- rlang::enquos(...)

  recipes::add_step(recipe,
                    step_select_cars_new(columns       = terms,
                                        outcome       = outcome,
                                        role          = role,
                                        trained       = trained,
                                        selected_vars = NULL,
                                        skip          = skip,
                                        id            = id))
}

## -----------------------------------------------------------------------------
## Step 2: Constructor
## -----------------------------------------------------------------------------

#' @export
step_select_cars_new <- function(columns,
                                 outcome,
                                 role,
                                 trained,
                                 selected_vars,
                                 skip,
                                 id) {

  out <- list(columns        = columns,
              outcome        = outcome,
              role           = role,
              trained        = trained,
              selected_vars  = selected_vars,
              skip           = skip,
              id             = id )

  class(out) <- c("step_select_cars", "step")
  return(out)

}

## -----------------------------------------------------------------------------
## Step 3: prep() method
## -----------------------------------------------------------------------------

#' @export
prep.step_select_cars <- function(x, training, info = NULL, ...) {

  ## ---------------------------------------------------------------------------
  ## Stage 1: Resolve spectral column names
  ## ---------------------------------------------------------------------------

  col_names <- recipes::recipes_eval_select(x$columns, training, info)

  if (!is.character(x$outcome) || length(x$outcome) != 1) {
    cli::cli_abort("The {.arg outcome} must be a single character string.")
  }

  if (!x$outcome %in% names(training)) {
    cli::cli_abort("Outcome column {.val {x$outcome}} not found in training data.")
  }

  outcome_vec  <- training[[x$outcome]]
  spectra_mat  <- as.matrix(training[, col_names, drop = FALSE])

  ## ---------------------------------------------------------------------------
  ## Stage 2: Cluster predictors before CARS
  ## ---------------------------------------------------------------------------

  cluster_spectral_predictors(spectra_mat,
                              k      = 150,
                              method = "correlation") -> cluster_result

  reduced_mat   <- cluster_result$reduced_mat
  cluster_map   <- cluster_result$cluster_map
  cluster_vars  <- cluster_result$selected_vars


  ## ---------------------------------------------------------------------------
  ## Stage 3: Run CARS
  ## ---------------------------------------------------------------------------

  # CARS parameters
  n_iterations   <- 50
  mc_ratio       <- 0.8
  n_components   <- min(5, floor(nrow(reduced_mat) / 3))
  
  # Initialize
  n_samples      <- nrow(reduced_mat)
  n_vars         <- ncol(reduced_mat)
  selected_vars  <- seq_len(n_vars)
  weights        <- rep(1, n_vars)
  rmsecv_values  <- numeric(n_iterations)
  decay          <- exp(log(2 / n_vars) / (n_iterations - 1))

  for (iter in 1:n_iterations) {

    # Safety check - need at least 2 variables for meaningful PLS
    if(length(selected_vars) < 2) {
      break
    }

    # Exponentially decreasing retention: n_vars → 2 over n_iterations
    n_keep         <- max(2, floor(n_vars * decay^(iter - 1)))
    n_keep         <- min(n_keep, length(selected_vars))
    
    # Monte Carlo sampling
    mc_samples     <- sample(n_samples, floor(n_samples * mc_ratio))
    
    # Weighted PLS on selected variables
    X_subset       <- reduced_mat[mc_samples, selected_vars, drop = FALSE]
    X_weighted     <- sweep(X_subset, 2, weights[selected_vars], "*")
    y_subset       <- outcome_vec[mc_samples]
    
    # Fit PLS model (need to create a data frame for the formula interface)
    pls_data       <- as.data.frame(X_weighted)
    pls_data$y     <- y_subset
    
    # Determine number of components safely
    n_comp_use <- min(n_components, ncol(X_weighted) - 1)
    if(n_comp_use < 1) n_comp_use <- 1
    
    pls_fit        <- pls::plsr(y ~ ., 
                                data       = pls_data,
                                ncomp      = n_comp_use,
                                validation = "LOO")
    
    # Get variable importance from regression coefficients
    coef_result    <- coef(pls_fit, ncomp = n_comp_use)
    
    # Handle matrix, array, and vector cases robustly
    var_importance <- tryCatch({
      if(is.array(coef_result) && length(dim(coef_result)) > 1 && dim(coef_result)[2] >= 1) {
        # Matrix case with at least 1 column
        abs(coef_result[, 1])
      } else {
        # Vector case or degenerate matrix
        abs(as.vector(coef_result))
      }
    }, error = function(e) {
      # Ultimate fallback - flatten everything
      abs(as.vector(coef_result))
    })
    
    # Update weights using EDF (Exponentially Decreasing Function)
    weights[selected_vars] <- weights[selected_vars] * var_importance
    
    # Select variables based on importance
    importance_threshold <- sort(var_importance, decreasing = TRUE)[min(n_keep, length(var_importance))]
    keep_indices        <- which(var_importance >= importance_threshold)
    selected_vars       <- selected_vars[keep_indices]
    
    # Calculate RMSECV for monitoring  
    rmsecv_values[iter] <- tryCatch({
      sqrt(mean(pls::RMSEP(pls_fit, estimate = "CV")$val[1, , ]))
    }, error = function(e) {
      # Fallback: use validation stats if available
      if(!is.null(pls_fit$validation)) {
        sqrt(mean((pls_fit$validation$PRESS)/(nrow(pls_data))))
      } else {
        # Ultimate fallback
        NA_real_
      }
    })
    
    # Early stopping if only one variable left
    if (length(selected_vars) <= 1) break
  }
  
  # Find optimal subset based on minimum RMSECV
  optimal_iter <- which.min(rmsecv_values[1:iter])
  
  # Check if we found a valid optimal iteration
  if (length(optimal_iter) == 0 || is.na(optimal_iter)) {

    # No valid iteration found — keep all predictors as safe fallback
    return(step_select_cars_new(
      columns       = col_names,
      outcome       = x$outcome,
      role          = x$role,
      trained       = TRUE,
      selected_vars = col_names,
      skip          = x$skip,
      id            = x$id
    ))

  }
  
  # Rerun to optimal iteration to get final variable set
  selected_vars <- seq_len(n_vars)
  weights      <- rep(1, n_vars)
  
  for (i in 1:optimal_iter) {
    # Safety check
    if(length(selected_vars) < 2) {
      break
    }
    
    n_keep        <- max(2, floor(n_vars * decay^(i - 1)))
    n_keep        <- min(n_keep, length(selected_vars))
    mc_samples    <- sample(n_samples, floor(n_samples * mc_ratio))
    
    X_subset      <- reduced_mat[mc_samples, selected_vars, drop = FALSE]
    X_weighted    <- sweep(X_subset, 2, weights[selected_vars], "*")
    y_subset      <- outcome_vec[mc_samples]
    
    # Create data frame for PLS
    pls_data      <- as.data.frame(X_weighted)
    pls_data$y    <- y_subset
    
    # Determine number of components safely
    n_comp_use    <- min(n_components, ncol(X_weighted) - 1)
    if(n_comp_use < 1) n_comp_use <- 1
    
    pls_fit       <- pls::plsr(y ~ .,
                               data       = pls_data,
                               ncomp      = n_comp_use,
                               validation = "LOO")
    
    coef_result   <- coef(pls_fit, ncomp = n_comp_use)
    
    # Handle matrix, array, and vector cases robustly
    var_importance <- tryCatch({
      if(is.array(coef_result) && length(dim(coef_result)) > 1 && dim(coef_result)[2] >= 1) {
        # Matrix case with at least 1 column
        abs(coef_result[, 1])
      } else {
        # Vector case or degenerate matrix
        abs(as.vector(coef_result))
      }
    }, error = function(e) {
      # Ultimate fallback - flatten everything
      abs(as.vector(coef_result))
    })
    
    weights[selected_vars] <- weights[selected_vars] * var_importance
    
    importance_threshold <- sort(var_importance, decreasing = TRUE)[min(n_keep, length(var_importance))]
    keep_indices        <- which(var_importance >= importance_threshold)
    selected_vars       <- selected_vars[keep_indices]
  }
  
  kept_cluster_vars <- cluster_vars[selected_vars]

  ## ---------------------------------------------------------------------------
  ## Stage 4: Map back to original wavenumbers
  ## ---------------------------------------------------------------------------

  kept_wavenumbers <- unique(unlist(cluster_map[kept_cluster_vars]))

  if (length(kept_wavenumbers) == 0) {
    cli::cli_alert_warning("No wavenumbers retained by CARS. Retaining all original predictors.")
    kept_wavenumbers <- col_names
  }

  ## ---------------------------------------------------------------------------
  ## Stage 5: Return trained step
  ## ---------------------------------------------------------------------------

  step_select_cars_new(columns        = col_names,
                      outcome        = x$outcome,
                      role           = x$role,
                      trained        = TRUE,
                      selected_vars  = kept_wavenumbers,
                      skip           = x$skip,
                      id             = x$id)
}


## -----------------------------------------------------------------------------
## Step 4: bake() method
## -----------------------------------------------------------------------------

#' @export
bake.step_select_cars <- function(object, new_data, ...) {

  if (is.null(object$selected_vars)) {
    cli::cli_abort("This step has not been trained yet. Please call `prep()` first.")
  }

  if (!all(object$selected_vars %in% names(new_data))) {
    cli::cli_abort("Some selected wavenumbers are missing in new_data.")
  }

  ## Retain: selected spectral vars + all non-spectral columns
  keep_cols <- c(object$selected_vars,
                 setdiff(names(new_data),
                         object$columns))

  dplyr::select(new_data, dplyr::all_of(keep_cols))

}

## -----------------------------------------------------------------------------
## Step 5: print() method
## -----------------------------------------------------------------------------

#' @export
print.step_select_cars <- function(x,
                                   width = max(20, options()$width - 30),
                                   ...) {

  cat("CARS-based spectral feature selection step\n")
  cat(glue::glue("• Outcome column: {x$outcome}\n"))

  if (x$trained) {
    cat(glue::glue("• {length(x$selected_vars)} wavenumbers retained after CARS\n"))
  } else {
    cat("• Step not yet trained\n")
  }

  invisible(x)
}

