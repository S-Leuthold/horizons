% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation-ensemble.R
\name{build_ensemble}
\alias{build_ensemble}
\title{Build Ensemble from Finalized Models}
\usage{
build_ensemble(
  finalized_models,
  input_data,
  variable,
  covariate_data = NULL,
  ensemble_method = "stacks",
  optimize_blending = FALSE,
  blend_metric = "rmse",
  test_prop = 0.2,
  seed = 123,
  allow_par = FALSE,
  n_cores = NULL,
  verbose = TRUE,
  output_dir = NULL
)
}
\arguments{
\item{finalized_models}{A tibble output from \code{finalize_top_workflows()} containing
columns: \code{wflow_id}, \code{workflow}, \code{cv_predictions}, and \code{cv_metrics} (or \code{metrics} for backward compatibility)}

\item{input_data}{A data frame containing the full dataset with predictors and response}

\item{variable}{Character string. Name of the response variable column in \code{input_data}}

\item{covariate_data}{Optional data frame containing additional covariate predictors.
Currently not used but reserved for future functionality. Default: \code{NULL}}

\item{ensemble_method}{Character string. Ensemble method to use:
\itemize{
\item \code{"stacks"}: Penalized regression meta-learner (default)
\item \code{"weighted_average"}: Simple weighted average by inverse RMSE
\item \code{"xgb_meta"}: XGBoost meta-learner for non-linear model blending
}}

\item{optimize_blending}{Logical. For stacks method, whether to optimize penalty
and mixture parameters via grid search. Default: \code{FALSE} (uses penalty = 0.01, mixture = 1)}

\item{blend_metric}{Character string. Metric to optimize during blending.
Options: \code{"rmse"} (default), \code{"mae"}, \code{"rsq"}, or any yardstick metric function name}

\item{test_prop}{Numeric between 0 and 1. Proportion of data to hold out for testing.
Default: \code{0.2} (20\% test set)}

\item{seed}{Integer. Random seed for train/test split reproducibility. Default: \code{123}}

\item{allow_par}{Logical. Enable parallel processing for model fitting. Default: \code{FALSE}}

\item{n_cores}{Integer or \code{NULL}. Number of cores for parallel processing.
If \code{NULL} and \code{allow_par = TRUE}, uses \code{detectCores() - 1}. Default: \code{NULL}}

\item{verbose}{Logical. Print progress messages and results summary. Default: \code{TRUE}}

\item{output_dir}{Character string or \code{NULL}. Directory path to save ensemble results.
Creates \verb{ensemble/} subdirectory with model and prediction files. Default: \code{NULL} (no saving)}
}
\value{
A list of class \code{"horizons_ensemble"} containing:
\itemize{
\item \code{ensemble_model}: The fitted ensemble model object (stacks or list with predict function)
\item \code{predictions}: Tibble with test set predictions (columns: \code{Observed}, \code{Predicted})
\item \code{metrics}: Tibble with test set performance metrics
\item \code{model_weights}: Data frame with model names and weights/coefficients
\item \code{individual_performance}: Tibble with individual model CV performance
\item \code{metadata}: List with ensemble configuration and performance summary
}
}
\description{
Creates an ensemble model from the output of \code{finalize_top_workflows()}.
Supports three ensemble methods:
\itemize{
\item \strong{Stacked ensembles}: Uses penalized regression meta-learner via tidymodels/stacks
\item \strong{Weighted average}: Weights models by inverse RMSE from cross-validation
\item \strong{XGBoost meta-learner}: Non-linear blending that can learn complex model interactions
}

\strong{Coming soon}: Additional meta-learner options including:
\itemize{
\item \code{"rf_meta"}: Random forest meta-learner for robust non-parametric blending
\item \code{"nn_meta"}: Neural network meta-learner for deep interaction modeling
}
}
\details{
\subsection{Error Handling}{

The function implements robust error handling for ensemble operations:

\strong{Critical operations (abort on failure)}:
\itemize{
\item Stack blending (\code{stacks::blend_predictions}) - Provides hints about CV predictions,
penalty/mixture grid size, and metric compatibility
\item Ensemble member fitting (\code{stacks::fit_members}) - Suggests checking training data,
workflow finalization, and parallel processing settings
\item XGBoost meta-learner training - Validates meta-features for missing/infinite values,
response variable type, and hyperparameter settings
}

\strong{Individual model fitting (graceful degradation)}:
\itemize{
\item Weighted average model fitting: Failed models are skipped, ensemble continues with N-1 models
\item XGBoost base model fitting: Failed models are skipped, meta-learner trains on remaining models
\item Minimum 2 models required for ensemble after filtering failures
\item Failed model attempts are logged with warnings, successful count reported
}

All error messages include actionable troubleshooting hints to help diagnose failures.
}
}
\examples{
\dontrun{
# Build ensemble from finalized models
ensemble <- build_ensemble(
  finalized_models = top_models,
  input_data = spectral_data,
  variable = "SOC",
  ensemble_method = "stacks",
  optimize_blending = TRUE,
  verbose = TRUE
)

# Access results
ensemble$metrics           # Test set performance
ensemble$model_weights     # Contributing models and weights
ensemble$metadata$improvement  # Improvement over best individual
}

}
