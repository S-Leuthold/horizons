% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation-ensemble.R
\name{build_ensemble}
\alias{build_ensemble}
\title{Build Ensemble from Finalized Models}
\usage{
build_ensemble(
  finalized_models,
  input_data,
  variable,
  covariate_data = NULL,
  ensemble_method = "stacks",
  optimize_ensemble = FALSE,
  blend_metric = "rmse",
  test_prop = 0.2,
  seed = 123,
  allow_par = FALSE,
  n_cores = NULL,
  verbose = TRUE,
  output_dir = NULL
)
}
\arguments{
\item{finalized_models}{A tibble output from \code{finalize_top_workflows()} containing
columns: \code{wflow_id}, \code{workflow}, \code{cv_predictions}, and \code{cv_metrics} (or \code{metrics} for backward compatibility)}

\item{input_data}{A data frame containing the full dataset with predictors and response}

\item{variable}{Character string. Name of the response variable column in \code{input_data}}

\item{covariate_data}{Optional data frame containing additional covariate predictors.
Currently not used but reserved for future functionality. Default: \code{NULL}}

\item{ensemble_method}{Character string. Ensemble method to use:
\itemize{
\item \code{"stacks"}: Penalized regression meta-learner (default)
\item \code{"weighted_average"}: Simple weighted average by inverse RMSE
\item \code{"xgb_meta"}: XGBoost meta-learner for non-linear model blending
}}

\item{optimize_ensemble}{Logical. Whether to optimize ensemble hyperparameters.
For stacks: grid search over penalty/mixture. For xgb_meta: CV-based early stopping.
Default: \code{FALSE} (uses fixed defaults for faster execution)}

\item{blend_metric}{Character string. Metric to optimize during blending.
Options: \code{"rmse"} (default), \code{"mae"}, \code{"rsq"}, or any yardstick metric function name}

\item{test_prop}{Numeric between 0 and 1. Proportion of data to hold out for testing.
Default: \code{0.2} (20\% test set)}

\item{seed}{Integer. Random seed for train/test split reproducibility. Default: \code{123}}

\item{allow_par}{Logical. Enable parallel processing for model fitting. Default: \code{FALSE}}

\item{n_cores}{Integer or \code{NULL}. Number of cores for parallel processing.
If \code{NULL} and \code{allow_par = TRUE}, uses \code{detectCores() - 1}. Default: \code{NULL}}

\item{verbose}{Logical. Print progress messages and results summary. Default: \code{TRUE}}

\item{output_dir}{Character string or \code{NULL}. Directory path to save ensemble results.
Creates \verb{ensemble/} subdirectory with model and prediction files. Default: \code{NULL} (no saving)}
}
\value{
A list of class \code{"horizons_ensemble"} containing:
\itemize{
\item \code{ensemble_model}: The fitted ensemble model object (stacks or list with predict function)
\item \code{predictions}: Tibble with test set predictions (columns: \code{Observed}, \code{Predicted})
\item \code{metrics}: Tibble with test set performance metrics
\item \code{model_weights}: Data frame with model names and weights/coefficients
\item \code{individual_performance}: Tibble with individual model CV performance
\item \code{metadata}: List with ensemble configuration and performance summary
}
}
\description{
Creates an ensemble model from the output of \code{finalize_top_workflows()}.
Supports three ensemble methods:
\itemize{
\item \strong{Stacked ensembles}: Uses penalized regression meta-learner via tidymodels/stacks
\item \strong{Weighted average}: Weights models by inverse RMSE from cross-validation
\item \strong{XGBoost meta-learner}: Non-linear blending that can learn complex model interactions
}

\strong{Coming soon}: Additional meta-learner options including:
\itemize{
\item \code{"rf_meta"}: Random forest meta-learner for robust non-parametric blending
\item \code{"nn_meta"}: Neural network meta-learner for deep interaction modeling
}
}
\details{
\subsection{XGBoost Meta-Learner Tuning}{

The \code{xgb_meta} method uses optimized defaults for meta-learning with few base models:

\strong{Default hyperparameters}:
\itemize{
\item \code{max_depth = 2}: Shallow trees prevent overfitting when blending ~6 base models
\item \code{eta = 0.05}: Conservative learning rate for stable convergence
\item \code{lambda = 1.0}, \code{alpha = 0.1}: Strong regularization to avoid memorizing CV fold patterns
}

\strong{When \code{optimize_ensemble = TRUE}}:
\itemize{
\item Runs 5-fold cross-validation with early stopping (max 500 rounds, stops after 20 no-improvement rounds)
\item Automatically finds optimal \code{nrounds} to prevent overfitting
\item Stores CV RMSE and optimal rounds in \code{metadata$xgb_cv_rmse} and \code{metadata$xgb_nrounds}
}

\strong{When \code{optimize_ensemble = FALSE}}:
\itemize{
\item Uses fixed \code{nrounds = 50} for faster execution
\item Suitable for quick prototyping or when model count is very small
}

The key insight: Meta-learning with few base models requires aggressive regularization.
Shallow trees and L1/L2 penalties prevent the meta-learner from overfitting to training quirks.
}

\subsection{Error Handling}{

The function implements robust error handling for ensemble operations:

\strong{Critical operations (abort on failure)}:
\itemize{
\item Stack blending (\code{stacks::blend_predictions}) - Provides hints about CV predictions,
penalty/mixture grid size, and metric compatibility
\item Ensemble member fitting (\code{stacks::fit_members}) - Suggests checking training data,
workflow finalization, and parallel processing settings
\item XGBoost meta-learner training - Validates meta-features for missing/infinite values,
response variable type, and hyperparameter settings
\item XGBoost CV (\code{xgb.cv}) - Checks for sufficient samples (need >= 25 for 5-fold CV)
}

\strong{Individual model fitting (graceful degradation)}:
\itemize{
\item Weighted average model fitting: Failed models are skipped, ensemble continues with N-1 models
\item XGBoost base model fitting: Failed models are skipped, meta-learner trains on remaining models
\item Minimum 2 models required for ensemble after filtering failures
\item Failed model attempts are logged with warnings, successful count reported
}

All error messages include actionable troubleshooting hints to help diagnose failures.
}
}
\examples{
\dontrun{
# Build stacked ensemble with optimization
ensemble_stacks <- build_ensemble(
  finalized_models = top_models,
  input_data = spectral_data,
  variable = "SOC",
  ensemble_method = "stacks",
  optimize_ensemble = TRUE,
  verbose = TRUE
)

# Build XGBoost meta-learner with CV tuning
ensemble_xgb <- build_ensemble(
  finalized_models = top_models,
  input_data = spectral_data,
  variable = "SOC",
  ensemble_method = "xgb_meta",
  optimize_ensemble = TRUE,  # Uses CV early stopping
  verbose = TRUE
)

# Access results
ensemble_xgb$metrics           # Test set performance
ensemble_xgb$model_weights     # Contributing models and importance
ensemble_xgb$metadata$xgb_nrounds  # Optimal number of boosting rounds
ensemble_xgb$metadata$improvement  # Improvement over best individual
}

}
