% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pipeline-evaluate.R
\name{evaluate}
\alias{evaluate}
\title{Evaluate Model Configurations}
\usage{
evaluate(
  x,
  metric = "rpd",
  prune = TRUE,
  prune_threshold = 1,
  workers = 1L,
  output_dir = NULL,
  seed = 307L,
  verbose = TRUE
)
}
\arguments{
\item{x}{A \code{horizons_data} object with \code{config$configs} populated by
\code{configure()}.}

\item{metric}{Character. Metric for ranking configurations. One of
\code{"rpd"}, \code{"rsq"}, \code{"rmse"}, \code{"rrmse"}, \code{"ccc"}, \code{"mae"}. Default
\code{"rpd"}.}

\item{prune}{Logical. If TRUE, skip Bayesian optimization for configs
whose grid-search RPD falls below \code{prune_threshold}. Default TRUE.}

\item{prune_threshold}{Numeric. RPD threshold for pruning. Configs with
grid-search RPD below this value skip Bayesian optimization but still
receive test-set metrics from grid-search best. Default 1.0 (the
"no better than the mean" line).}

\item{workers}{Integer. Total number of cores to use. Default 1L
(sequential). When \code{workers > 1}, evaluate() automatically splits cores
between outer parallelism (across configs) and inner parallelism (CV
folds). The split is: \code{inner = min(workers, cv_folds)}, \code{outer = floor(workers / inner)}. evaluate() manages its own \code{future::plan()} and
restores the previous plan on exit.}

\item{output_dir}{Character or NULL. If provided, checkpoint results to
disk after each config. Enables resuming interrupted runs. Default NULL
(no checkpointing).}

\item{seed}{Integer. Random seed for train/test split and CV folds.
Default 307L.}

\item{verbose}{Logical. Print progress tree to console. Default TRUE.}
}
\value{
A \code{horizons_eval} object (inherits from \code{horizons_data}) with
\code{evaluation$results}, \code{evaluation$best_config}, \code{evaluation$split}, and
associated metadata populated.
}
\description{
Evaluates all model configurations from \code{configure()} against held-out test
data. Each configuration is tuned via grid search (optionally followed by
Bayesian optimization), evaluated on the test set, and ranked by the
specified metric. The best configuration is stored for downstream use by
\code{fit()}.
}
