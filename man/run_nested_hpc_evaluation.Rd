% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation-nested_parallel_orchestrator.R
\name{run_nested_hpc_evaluation}
\alias{run_nested_hpc_evaluation}
\title{Nested Parallel HPC Evaluation Orchestrator}
\usage{
run_nested_hpc_evaluation(
  config,
  input_data,
  covariate_data = NULL,
  variable,
  total_cores = 50,
  outer_workers = NULL,
  inner_workers = NULL,
  memory_per_model_gb = 15,
  output_dir = NULL,
  grid_size_eval = 10,
  bayesian_iter_eval = 15,
  cv_folds_eval = 5,
  checkpoint_dir = NULL,
  resume = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{config}{Model configuration tibble with columns:
\itemize{
\item model: Model type (e.g., "random_forest", "cubist", "xgboost")
\item transformation: Response transformation
\item preprocessing: Spectral preprocessing method
\item feature_selection: Feature selection method
\item covariates: List-column of covariate sets
\item include_covariates: Logical flag for covariate inclusion
}}

\item{input_data}{Spectral data tibble with Sample_ID and wavenumber columns}

\item{covariate_data}{Optional covariate tibble matched by Sample_ID}

\item{variable}{Character. Name of the response variable}

\item{total_cores}{Integer. Total cores to use (default: 50)}

\item{outer_workers}{Integer. Number of concurrent models (auto-calculated if NULL)}

\item{inner_workers}{Integer. Workers per model (auto-calculated if NULL)}

\item{memory_per_model_gb}{Numeric. Expected memory per model in GB (default: 15)}

\item{output_dir}{Character. Output directory path}

\item{grid_size_eval}{Integer. Grid search size (default: 10)}

\item{bayesian_iter_eval}{Integer. Bayesian optimization iterations (default: 15)}

\item{cv_folds_eval}{Integer. Cross-validation folds (default: 5)}

\item{checkpoint_dir}{Character. Checkpoint directory path}

\item{resume}{Logical. Resume from checkpoints (default: TRUE)}

\item{verbose}{Logical. Print progress messages (default: TRUE)}
}
\value{
A tibble with evaluation results for all models
}
\description{
Implements two-tier parallelization for optimal HPC resource utilization.
Outer loop runs N model configurations concurrently, each with M inner workers
for tuning, where N Ã— M = total_cores. This dramatically improves throughput
compared to sequential model processing.
}
