% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation-hpc_orchestrator.R
\name{run_hpc_evaluation}
\alias{run_hpc_evaluation}
\title{HPC-Optimized Model Evaluation Orchestrator}
\usage{
run_hpc_evaluation(
  config,
  input_data,
  covariate_data = NULL,
  variable,
  output_dir = NULL,
  n_workers = 50,
  grid_size_eval = 10,
  bayesian_iter_eval = 15,
  cv_folds_eval = 10,
  retrain_top_models = TRUE,
  number_models_retained = 15,
  grid_size_final = 25,
  bayesian_iter_final = 20,
  cv_folds_final = 15,
  pruning = FALSE,
  checkpoint_dir = NULL,
  resume = TRUE,
  verbose = TRUE,
  nested_parallel = FALSE,
  outer_workers = NULL,
  inner_workers = NULL
)
}
\arguments{
\item{config}{A tibble of model configurations to evaluate. Must include columns:
\itemize{
\item \code{model}: Model type (e.g., "random_forest", "cubist", "xgboost")
\item \code{transformation}: Response transformation (e.g., "No Transformation", "Log Transformation")
\item \code{preprocessing}: Spectral preprocessing (e.g., "raw", "snv", "deriv1")
\item \code{feature_selection}: Feature selection method (e.g., "none", "pca", "boruta", "cars")
\item \code{covariates}: List-column of covariate sets
\item \code{include_covariates}: Logical flag for covariate inclusion
}}

\item{input_data}{A tibble with preprocessed spectral data, including \code{Sample_ID},
wavenumber columns, and the target response variable}

\item{covariate_data}{Optional tibble of predicted covariates matched by \code{Sample_ID}.
Required if \code{include_covariates = TRUE} in any row of \code{config}}

\item{variable}{Character. Name of the response variable (must exist in \code{input_data})}

\item{output_dir}{Path to output directory. Defaults to timestamped folder under \code{variable}}

\item{n_workers}{Integer. Number of parallel workers (1-190). Defaults to 50}

\item{grid_size_eval}{Integer. Number of combinations in initial grid search (default = 10)}

\item{bayesian_iter_eval}{Integer. Number of Bayesian optimization iterations (default = 15)}

\item{cv_folds_eval}{Integer. Number of CV folds during evaluation phase (default = 5)}

\item{retrain_top_models}{Logical. Whether to refit top N models after screening (default = TRUE)}

\item{number_models_retained}{Integer. Number of top models to refit if \code{retrain_top_models = TRUE} (default = 15)}

\item{grid_size_final}{Integer. Grid size for refitting stage (default = 25)}

\item{bayesian_iter_final}{Integer. Bayesian iterations during refitting (default = 20)}

\item{cv_folds_final}{Integer. Number of CV folds for refitting phase (default = 15)}

\item{pruning}{Logical. Whether to enable early pruning of poor configurations (default = FALSE)}

\item{checkpoint_dir}{Character. Directory for checkpoint files. If NULL, uses \verb{output_dir/checkpoints/}}

\item{resume}{Logical. Whether to resume from existing checkpoints (default = TRUE)}

\item{verbose}{Logical. Print detailed progress messages (default = TRUE)}

\item{nested_parallel}{Logical. Use nested parallelization (default = FALSE)}

\item{outer_workers}{Integer. Number of concurrent models for nested mode (optional)}

\item{inner_workers}{Integer. Workers per model for nested mode (optional)}
}
\value{
If \code{retrain_top_models = FALSE}, returns a tibble summarizing evaluation metrics
for each configuration. If \code{TRUE}, returns a list with:
\itemize{
\item \strong{full_summary}: A tibble of metrics and metadata from initial evaluation
\item \strong{refit_summary}: A tibble of metrics from the final refitting stage
}
}
\description{
Orchestrates sequential model evaluation with parallel tuning operations optimized
for HPC environments with many cores (50-190). Each model configuration is evaluated
sequentially, but within each model, grid search and Bayesian optimization leverage
all available workers for maximum throughput.

This function replaces \code{run_model_evaluation()} for HPC environments, providing:
\itemize{
\item Configurable worker allocation (1-190 cores)
\item Sequential model processing with parallel fits
\item Work-stealing for efficient resource utilization
\item Memory-aware processing with minimal overhead
\item Comprehensive progress tracking and checkpointing
}
}
\details{
\subsection{Parallel Processing Strategy}{

The orchestrator implements a two-level parallelization strategy optimized for HPC:

\strong{Level 1: Sequential Model Processing}
\itemize{
\item Each model configuration runs sequentially to avoid memory competition
\item Allows full resource dedication to each model
\item Simplifies debugging and monitoring
}

\strong{Level 2: Parallel Tuning Operations}
\itemize{
\item Grid search: Parallelizes across hyperparameter combinations
\item Bayesian optimization: Parallelizes across CV folds within each iteration
\item Uses work-stealing for automatic load balancing
}
}

\subsection{Memory Management}{
\itemize{
\item Minimal garbage collection (only when needed)
\item Automatic cleanup between models
\item Memory monitoring with configurable thresholds
\item Efficient data passing to workers
}
}

\subsection{Checkpointing and Recovery}{
\itemize{
\item Saves progress after each model completion
\item Automatic resume from last successful model
\item Preserves random seeds for reproducibility
\item Checkpoint files in node-local storage for speed
}
}
}
\examples{
\dontrun{
# Run on HPC with 50 cores
results <- run_hpc_evaluation(
  config             = model_config_grid,
  input_data         = spectral_data,
  covariate_data     = predicted_covs,
  variable           = "MAOM_C_g_kg",
  n_workers          = 50,
  grid_size_eval     = 10,
  bayesian_iter_eval = 15
)

# Run with more cores for larger grids
results <- run_hpc_evaluation(
  config             = large_config_grid,
  input_data         = spectral_data,
  variable           = "SOC_g_kg",
  n_workers          = 190,
  grid_size_eval     = 50,
  bayesian_iter_eval = 30
)
}

}
\seealso{
\code{\link{evaluate_model_fit_parallel}}, \code{\link{manage_worker_pool}},
\code{\link{run_model_evaluation}}, \code{\link{evaluate_single_model_parallel}}
}
