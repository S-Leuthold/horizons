# M3.3: CV+ Conformal Calibration - Implementation Specification

## Current Status (End of Session 6)

**What Works:**
- ✅ Residual-based UQ with OOF point predictions
- ✅ Quantile model training on residuals
- ✅ Coverage: 90.2% (excellent base model!)
- ✅ Interval width: 1.43 pH (37% narrower than library-based)

**What's Missing:**
- ❌ Proper CV+ conformal calibration with OOF quantile predictions
- ❌ Mathematical coverage guarantee

**Current Placeholder:**
- `calculate_conformal_margin()` exists but returns c_alpha=0
- Uses in-sample quantile predictions (optimistic bias)
- Needs replacement with proper CV+ approach

---

## Statistical Validity Concern

**Problem with Current Approach:**
```r
# Step 1: Train quantile model on ALL residuals
quantile_model <- train_quantile_model(residual_data)  # Sees all 1600 samples

# Step 2: Predict quantiles on SAME 1600 samples (in-sample!)
quantiles <- predict_quantiles(quantile_model, residual_data)

# Step 3: Compute scores using OOF point + in-sample quantiles
scores <- f(point_OOF, quantile_IN_SAMPLE, truth)
```

**Why This is Problematic:**
- Point predictions are OOF (unbiased) ✓
- Quantile predictions are in-sample (optimistic) ✗
- Mismatch introduces bias in conformity scores
- c_alpha will be underestimated (intervals too narrow)
- Same issue we just fixed for residuals!

**Impact:**
- Defeats the statistical guarantee of conformal prediction
- May achieve <90% coverage in practice
- Not production-ready for service labs

---

## Solution: True CV+ Conformal with OOF Quantiles

### **Approach:**

Use the SAME CV folds for BOTH point and quantile models to get matched OOF predictions.

### **Implementation Steps:**

#### **Step 1: Restructure Training to Use Shared CV Folds**

**Current:**
```r
# Point model creates its own folds
point_result <- train_and_score_config(...)  # Has internal CV

# Quantile model creates different folds
quantile_model <- train_quantile_model(...)  # Has internal CV
```

**Needed:**
```r
# Create CV folds ONCE
cv_folds <- rsample::vfold_cv(cluster_data, v = 10, strata = Response)

# Pass same folds to both models
point_result <- train_and_score_config(..., resamples = cv_folds)
quantile_result <- train_quantile_model(..., resamples = cv_folds)

# Now OOF predictions are MATCHED (same fold assignments)
```

#### **Step 2: Extract OOF Quantile Predictions**

**Challenge:**
`tune_grid()` on quantile models returns `.pred` (point prediction), not quantile predictions.

**Solution:**
After tuning, manually predict quantiles on each CV fold's assessment set:

```r
## In train_quantile_model(), after selecting best_params:

# Finalize model with best hyperparameters
quantile_model_finalized <- finalize_workflow(wf, best_params)

# For each CV fold, predict quantiles on held-out samples
oof_quantile_list <- list()

for (fold_id in 1:nrow(cv_folds)) {

  # Get training and assessment indices for this fold
  fold_data <- rsample::analysis(cv_folds$splits[[fold_id]])
  fold_assess <- rsample::assessment(cv_folds$splits[[fold_id]])

  # Fit on fold training data
  fold_model <- parsnip::fit(quantile_model_finalized, fold_data)

  # Predict quantiles on fold assessment (OOF!)
  fold_quantiles <- predict_quantiles(
    fold_model,
    fold_assess,
    quantiles = c(0.05, 0.95)
  )

  # Store with row indices
  fold_quantiles$.row <- fold_assess$.row  # Assuming we track row indices
  oof_quantile_list[[fold_id]] <- fold_quantiles
}

# Combine all OOF quantile predictions
oof_quantiles <- bind_rows(oof_quantile_list) %>% arrange(.row)

# Return with final model
list(
  workflow = fitted_wf,
  cv_quantile_predictions = oof_quantiles  # OOF quantiles!
)
```

#### **Step 3: Compute Conformal Margin with Matched OOF Predictions**

```r
## In calculate_conformal_margin():

# Now we have:
# - point_cv_preds: OOF point predictions (from point model CV)
# - quantile_cv_preds: OOF quantile predictions (q05 and q95 residuals)
# - Both from SAME CV folds, SAME samples

# Construct OOF intervals
calibration_data <- point_cv_preds %>%
  left_join(quantile_cv_preds, by = ".row") %>%
  mutate(
    interval_lower = point_pred + resid_q05,  # Both OOF!
    interval_upper = point_pred + resid_q95
  )

# Compute nonconformity scores
scores <- pmax(
  calibration_data$interval_lower - calibration_data$Response,
  calibration_data$Response - calibration_data$interval_upper
)

# Calculate c_alpha
c_alpha <- quantile(scores, probs = 0.90)
```

**Benefits:**
- ✅ Statistically valid (both predictions OOF)
- ✅ Uses ALL training data for calibration
- ✅ Stable c_alpha estimates
- ✅ True coverage guarantee

---

## Implementation Checklist for Session 7

### **Phase 1: Shared CV Infrastructure** (2-3 hrs)

- [ ] Modify `train_and_score_config()` to accept optional `resamples` parameter
  - If provided, use those folds instead of creating new ones
  - Return fold assignments for downstream use

- [ ] Modify `train_quantile_model()` to accept optional `resamples` parameter
  - Same pattern as point model

- [ ] Update `train_cluster_models_with_uq()` to create folds once:
  ```r
  cv_folds <- vfold_cv(cluster_data, v = cv_folds, strata = Response)
  point_result <- train_and_score_config(..., resamples = cv_folds)
  quantile_result <- train_quantile_model(..., resamples = cv_folds)
  ```

### **Phase 2: OOF Quantile Extraction** (3-4 hrs)

- [ ] In `train_quantile_model()`, after tuning:
  - Loop through CV folds
  - Fit finalized model on each fold's training set
  - Predict quantiles (q05, q95) on each fold's assessment set
  - Combine into single OOF quantile dataframe

- [ ] Return OOF quantiles:
  ```r
  list(
    workflow = fitted_wf,
    cv_quantiles = oof_quantile_predictions
  )
  ```

- [ ] Update callers to handle list return (extract `$workflow`)

### **Phase 3: Proper Conformal Calibration** (1-2 hrs)

- [ ] Rewrite `calculate_conformal_margin()`:
  - Join OOF point + OOF quantiles by `.row`
  - Construct intervals from matched OOF predictions
  - Compute nonconformity scores
  - Calculate c_alpha as (1-alpha) quantile

- [ ] Validate:
  - Check coverage on calibration set ≈ alpha (should be ~10% outside)
  - Ensure c_alpha is reasonable (expect 0.1-0.3 pH for well-calibrated base model)

### **Phase 4: Testing & Validation** (2-3 hrs)

- [ ] Test with debug script:
  - Verify c_alpha > 0 (should add small margin)
  - Check base vs conformal coverage difference

- [ ] Test with full OSSL:
  - 80/20 split: train/test
  - Train with CV+ conformal
  - Validate coverage on test set ≈ 90% (target 88-92%)

- [ ] Edge cases:
  - Small clusters (n < 200)
  - Different properties (pH, clay, OC)
  - Verify c_alpha varies appropriately

### **Phase 5: Documentation** (1 hr)

- [ ] Update function roxygen with proper CV+ description
- [ ] Document computational cost (2× CV loops)
- [ ] Add warning about memory usage with large clusters
- [ ] Mark M3.3 complete in roadmap

---

## Alternative: Simple Holdout Conformal (If CV+ Too Complex)

If CV+ proves too complex or slow, fall back to:

```r
# Split cluster data:
# 60% train models
# 20% conformal calibration (holdout, never seen by models)
# 20% final test

train_60 <- cluster_data[1:960, ]
calib_20 <- cluster_data[961:1280, ]
test_20  <- cluster_data[1281:1600, ]

# Train on 60%
models <- train_cluster_models_with_uq(train_60, ...)

# Predict on calibration 20% (OOF!)
calib_preds <- predict_with_uq(models, calib_20)

# Compute scores
scores <- pmax(
  calib_preds$.pred_lower - calib_20$Response,
  calib_20$Response - calib_preds$.pred_upper
)

# c_alpha
c_alpha <- quantile(scores, 0.90)

# Validate on test 20%
test_preds <- predict_with_uq(models, test_20, c_alpha = c_alpha)
coverage <- mean(test_20$Response >= test_preds$.pred_lower &
                test_20$Response <= test_preds$.pred_upper)
# Should be ~90%
```

**Trade-offs:**
- ✅ Simpler implementation (no fold matching)
- ✅ Statistically valid (calibration set is holdout)
- ❌ Less data for training (960 vs 1600 samples)
- ❌ Less stable c_alpha (320 vs 1600 calibration samples)

**Recommendation:** Try CV+ first, fall back to holdout if implementation takes >6 hours.

---

## Estimated Time

**CV+ Implementation:** 8-12 hours (spread over 2 sessions)
**Holdout Implementation:** 2-3 hours (single session)

**For Production Service Lab:** CV+ is worth the investment (more data, better calibration)

---

## Notes for Session 7

- Current code has conformal infrastructure in place (just needs OOF quantiles)
- `save_pred = TRUE` already enabled for both models
- Main task: Extract and match OOF quantile predictions across folds
- Test rigorously: this is the coverage guarantee layer!
