---
title: "Advanced Modeling with horizons"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Advanced Modeling with horizons}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(horizons)
library(dplyr)
library(ggplot2)
library(patchwork)
```

# Introduction

This vignette covers advanced modeling techniques in the `horizons` package, including:

- Custom model specifications and tuning strategies
- Ensemble stacking and model interpretation
- Performance optimization and parallel processing
- Advanced preprocessing and feature selection
- Error handling and troubleshooting

# Advanced Model Specifications

## Custom Model Parameters

While `horizons` provides sensible defaults, you can customize model specifications for specific use cases:

```{r custom_models, eval=FALSE}
# Create custom model specifications
custom_models <- list(
  # High-performance Random Forest
  "rf_optimized" = parsnip::rand_forest(
    trees = 1000,
    mtry = parsnip::tune(),
    min_n = parsnip::tune()
  ) %>%
    parsnip::set_engine("ranger", 
                       importance = "impurity",
                       num.threads = parallel::detectCores()) %>%
    parsnip::set_mode("regression"),
  
  # Regularized Cubist
  "cubist_regularized" = parsnip::cubist_rules(
    committees = parsnip::tune(),
    neighbors = parsnip::tune()
  ) %>%
    parsnip::set_engine("Cubist") %>%
    parsnip::set_mode("regression")
)

# Use in configuration
project_configs <- create_project_configurations(
  project_data = project_data,
  models = names(custom_models),
  custom_models = custom_models,
  # ... other parameters
)
```

## Advanced Hyperparameter Tuning

### Bayesian Optimization Strategies

```{r bayes_tuning, eval=FALSE}
# Custom Bayesian optimization control
bayes_control <- tune::control_bayes(
  no_improve = 10,        # Stop after 10 iterations without improvement
  time_limit = 30,        # Maximum 30 minutes per model
  uncertainty = 0.01,     # Exploration parameter
  save_pred = TRUE,       # Save predictions for ensemble
  save_workflow = TRUE,   # Save workflow objects
  verbose = TRUE
)

# Use in model evaluation
run_model_evaluation(
  config = project_configs$project_configurations,
  input_data = project_data,
  covariate_data = project_configs$covariate_data,
  variable = "MAOM_C_g_kg",
  output_dir = "./advanced_results",
  grid_size_eval = 20,      # Larger initial grid
  bayesian_iter_eval = 30,  # More Bayesian iterations
  bayes_control = bayes_control,
  parallel_cores = 8        # Parallel processing
)
```

### Custom Tuning Grids

```{r custom_grids, eval=FALSE}
# Define custom parameter grids
rf_grid <- expand.grid(
  mtry = seq(10, 100, by = 10),
  min_n = c(2, 5, 10, 20),
  trees = c(500, 1000, 1500)
)

xgb_grid <- expand.grid(
  trees = c(100, 500, 1000),
  tree_depth = c(3, 6, 10),
  learn_rate = c(0.01, 0.1, 0.3),
  mtry = seq(10, 50, by = 10),
  min_n = c(5, 10, 20),
  loss_reduction = c(0, 0.1, 1),
  sample_size = c(0.8, 0.9, 1.0)
)

# Use with specific models
custom_grids <- list(
  "random_forest" = rf_grid,
  "xgboost" = xgb_grid
)
```

# Ensemble Stacking Deep Dive

## Understanding the Stacking Process

The `build_ensemble_stack()` function implements a sophisticated stacking approach:

1. **Candidate Generation**: Top models generate predictions on new CV folds
2. **Meta-learning**: A penalized regression model learns optimal combinations
3. **Constraint Handling**: Ensures non-negative weights and prevents overfitting

```{r ensemble_details, eval=FALSE}
# Detailed ensemble configuration
ensemble_config <- list(
  # Model selection criteria
  filter_metric = "rrmse",
  n_best = 15,              # More candidate models
  
  # Stacking parameters
  penalty_range = c(-5, 0), # Regularization strength
  mixture_range = c(0, 1),  # Elastic net mixing
  
  # Cross-validation
  cv_folds = 5,
  cv_repeats = 3,
  
  # Performance thresholds
  min_improvement = 0.01,   # Minimum ensemble improvement
  correlation_cutoff = 0.95 # Remove highly correlated candidates
)

final_ensemble <- build_ensemble_stack(
  results_dir = "./advanced_results",
  input_data = project_data,
  variable = "MAOM_C_g_kg",
  config = ensemble_config
)
```

## Model Interpretation and Diagnostics

### Ensemble Composition Analysis

```{r ensemble_analysis, eval=FALSE}
# Extract ensemble weights
ensemble_weights <- final_ensemble$ensemble_fit %>%
  stacks::collect_parameters() %>%
  filter(coefs != 0) %>%
  arrange(desc(coefs))

print(ensemble_weights)

# Visualize model contributions
plot_ensemble_weights <- function(weights) {
  weights %>%
    mutate(member = reorder(member, coefs)) %>%
    ggplot(aes(x = member, y = coefs)) +
    geom_col(fill = "steelblue", alpha = 0.7) +
    coord_flip() +
    labs(
      title = "Ensemble Model Weights",
      x = "Model Configuration",
      y = "Weight in Ensemble"
    ) +
    theme_minimal()
}

plot_ensemble_weights(ensemble_weights)
```

### Cross-Validation Performance

```{r cv_performance, eval=FALSE}
# Analyze CV performance across folds
cv_results <- final_ensemble$ensemble_fit %>%
  tune::collect_metrics(summarize = FALSE)

cv_plot <- cv_results %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = id, y = .estimate)) +
  geom_boxplot(fill = "lightblue", alpha = 0.7) +
  geom_point(position = position_jitter(width = 0.2)) +
  labs(
    title = "Cross-Validation Performance",
    x = "CV Fold",
    y = "RMSE"
  ) +
  theme_minimal()

print(cv_plot)
```

### Variable Importance

```{r variable_importance, eval=FALSE}
# Extract variable importance from ensemble members
extract_importance <- function(ensemble_fit) {
  # Get member weights
  weights <- stacks::collect_parameters(ensemble_fit) %>%
    filter(coefs != 0)
  
  # Extract importance from each member
  importance_list <- map(weights$member, function(member_name) {
    member_fit <- ensemble_fit$member_fits[[member_name]]
    
    # Extract importance (method depends on model type)
    if (inherits(member_fit, "ranger")) {
      importance_scores <- member_fit$variable.importance
    } else if (inherits(member_fit, "xgb.Booster")) {
      importance_scores <- xgboost::xgb.importance(model = member_fit)
    }
    # Add other model types as needed
    
    return(importance_scores)
  })
  
  # Combine and weight by ensemble contribution
  # Implementation depends on specific requirements
}

# Use function
ensemble_importance <- extract_importance(final_ensemble$ensemble_fit)
```

# Performance Optimization

## Parallel Processing Configuration

```{r parallel_setup, eval=FALSE}
# Setup parallel processing
library(future)
library(furrr)

# Configure parallel backend
plan(multisession, workers = parallel::detectCores() - 1)

# Set furrr options for progress tracking
options(furrr.progress = TRUE)

# Memory-efficient processing
options(future.globals.maxSize = 2 * 1024^3)  # 2GB limit
```

## Memory Management

```{r memory_management, eval=FALSE}
# Monitor memory usage during modeling
monitor_memory <- function() {
  gc()  # Force garbage collection
  mem_used <- pryr::mem_used()
  cli::cli_inform("Current memory usage: {mem_used}")
  return(mem_used)
}

# Use during model evaluation
pre_memory <- monitor_memory()

run_model_evaluation(
  # ... parameters ...
  memory_monitor = TRUE,
  cleanup_intermediate = TRUE  # Remove intermediate objects
)

post_memory <- monitor_memory()
cli::cli_inform("Memory change: {post_memory - pre_memory}")
```

## Caching Strategies

```{r caching, eval=FALSE}
# Configure caching for reproducibility
cache_config <- list(
  # Enable caching
  use_cache = TRUE,
  cache_dir = "./model_cache",
  
  # Cache validation
  cache_timeout = 24 * 3600,  # 24 hours
  force_refresh = FALSE,
  
  # Selective caching
  cache_predictions = TRUE,
  cache_models = TRUE,
  cache_preprocessed = TRUE
)

# Apply to workflow
project_configs <- create_project_configurations(
  project_data = project_data,
  cache_config = cache_config,
  # ... other parameters
)
```

# Advanced Preprocessing Techniques

## Custom Recipe Steps

```{r custom_recipes, eval=FALSE}
# Create custom preprocessing recipe
advanced_recipe <- function(data, response_var) {
  recipes::recipe(data, formula = as.formula(paste(response_var, "~ ."))) %>%
    
    # Spectral preprocessing
    step_transform_spectra(
      starts_with("X"),
      method = "snv_deriv1",
      window_size = 9,
      derivative_order = 1
    ) %>%
    
    # Feature selection
    step_select_correlation(
      starts_with("X"),
      threshold = 0.95,
      method = "spearman"
    ) %>%
    
    # Dimensionality reduction
    recipes::step_pca(
      starts_with("X"),
      threshold = 0.99,
      prefix = "PC"
    ) %>%
    
    # Covariate integration
    step_add_covariates(
      soil_vars = c("pH", "Clay_pct", "SOC_pct"),
      climate_vars = c("MAT", "MAP", "PET"),
      interaction_terms = TRUE
    ) %>%
    
    # Response transformation
    recipes::step_log(all_outcomes(), base = exp(1)) %>%
    
    # Final normalization
    recipes::step_normalize(all_predictors())
}

# Use in model building
custom_workflow <- workflows::workflow() %>%
  workflows::add_recipe(advanced_recipe(project_data, "MAOM_C_g_kg")) %>%
  workflows::add_model(parsnip::rand_forest(trees = 1000))
```

## Spectral Quality Assessment

```{r quality_assessment, eval=FALSE}
# Implement spectral quality checks
assess_spectral_quality <- function(spectral_data) {
  
  # Calculate quality metrics
  quality_metrics <- spectral_data %>%
    select(starts_with("X")) %>%
    rowwise() %>%
    summarise(
      # Signal-to-noise ratio
      snr = mean(c_across(everything())) / sd(c_across(everything())),
      
      # Spectral range
      range_ratio = (max(c_across(everything())) - min(c_across(everything()))) / 
                    mean(c_across(everything())),
      
      # Missing values
      missing_pct = sum(is.na(c_across(everything()))) / n(),
      
      # Outlier detection (Mahalanobis distance)
      mahal_dist = mahalanobis(c_across(everything()), 
                              colMeans(c_across(everything()), na.rm = TRUE),
                              cov(c_across(everything()), use = "complete.obs"))
    ) %>%
    ungroup()
  
  # Flag problematic spectra
  quality_flags <- quality_metrics %>%
    mutate(
      low_snr = snr < quantile(snr, 0.05, na.rm = TRUE),
      high_outlier = mahal_dist > quantile(mahal_dist, 0.95, na.rm = TRUE),
      high_missing = missing_pct > 0.05,
      quality_flag = low_snr | high_outlier | high_missing
    )
  
  return(quality_flags)
}

# Apply quality assessment
quality_results <- assess_spectral_quality(project_data)
clean_data <- project_data[!quality_results$quality_flag, ]
```

# Error Handling and Troubleshooting

## Robust Model Evaluation

```{r robust_evaluation, eval=FALSE}
# Enhanced error handling
robust_model_evaluation <- function(config, input_data, ...) {
  
  # Setup error tracking
  error_log <- list()
  successful_configs <- list()
  
  # Process each configuration with error handling
  for (i in seq_len(nrow(config))) {
    
    tryCatch({
      # Attempt model evaluation
      result <- safely_execute(
        expr = {
          evaluate_model_config(
            config_row = config[i, ],
            input_data = input_data,
            ...
          )
        },
        default_value = NULL,
        error_message = "Failed at configuration {i}",
        log_error = TRUE,
        capture_trace = TRUE
      )
      
      if (!is.null(result)) {
        successful_configs[[length(successful_configs) + 1]] <- result
        cli::cli_alert_success("✓ Configuration {i} completed successfully")
      }
      
    }, error = function(e) {
      error_log[[length(error_log) + 1]] <- list(
        config_index = i,
        error_message = as.character(e),
        timestamp = Sys.time()
      )
      cli::cli_alert_danger("✗ Configuration {i} failed: {e$message}")
    })
  }
  
  # Return results and errors
  return(list(
    successful_results = successful_configs,
    error_log = error_log,
    success_rate = length(successful_configs) / nrow(config)
  ))
}
```

## Diagnostic Plots

```{r diagnostics, eval=FALSE}
# Create comprehensive diagnostic plots
create_diagnostic_plots <- function(model_results) {
  
  # Performance distribution
  p1 <- model_results %>%
    ggplot(aes(x = rrmse)) +
    geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
    geom_vline(aes(xintercept = median(rrmse)), 
               color = "red", linetype = "dashed") +
    labs(title = "RRMSE Distribution", x = "RRMSE (%)", y = "Count")
  
  # Model comparison
  p2 <- model_results %>%
    ggplot(aes(x = reorder(model_type, rrmse), y = rrmse)) +
    geom_boxplot(fill = "lightblue", alpha = 0.7) +
    coord_flip() +
    labs(title = "Model Performance by Type", x = "Model", y = "RRMSE (%)")
  
  # Preprocessing effects
  p3 <- model_results %>%
    ggplot(aes(x = preprocessing, y = rrmse, fill = preprocessing)) +
    geom_boxplot(alpha = 0.7) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Preprocessing Effects", x = "Method", y = "RRMSE (%)") +
    guides(fill = "none")
  
  # Covariate importance
  p4 <- model_results %>%
    group_by(covariate_set) %>%
    summarise(mean_rrmse = mean(rrmse), .groups = "drop") %>%
    ggplot(aes(x = reorder(covariate_set, mean_rrmse), y = mean_rrmse)) +
    geom_col(fill = "darkgreen", alpha = 0.7) +
    coord_flip() +
    labs(title = "Covariate Set Performance", x = "Covariate Set", y = "Mean RRMSE (%)")
  
  # Combine plots
  combined_plot <- (p1 + p2) / (p3 + p4)
  
  return(combined_plot)
}

# Generate diagnostic plots
diagnostic_plots <- create_diagnostic_plots(evaluation_results)
print(diagnostic_plots)
```

# Advanced Validation Strategies

## Spatial Cross-Validation

```{r spatial_cv, eval=FALSE}
# Implement spatial cross-validation for geographic data
spatial_cv_split <- function(data, coords_cols = c("longitude", "latitude"), 
                            k_folds = 5) {
  
  # Extract coordinates
  coords <- data %>%
    select(all_of(coords_cols)) %>%
    as.matrix()
  
  # Perform spatial clustering
  spatial_clusters <- cluster::pam(coords, k = k_folds)
  
  # Create fold assignments
  fold_assignments <- spatial_clusters$clustering
  
  # Create rsample splits
  spatial_splits <- map(1:k_folds, function(fold) {
    in_fold <- which(fold_assignments == fold)
    rsample::manual_rset(
      splits = list(rsample::make_splits(
        x = list(analysis = setdiff(1:nrow(data), in_fold),
                assessment = in_fold),
        data = data
      )),
      ids = paste0("Spatial", sprintf("%02d", fold))
    )
  }) %>%
    reduce(rbind)
  
  return(spatial_splits)
}

# Use in model evaluation
spatial_folds <- spatial_cv_split(project_data)
```

## Temporal Validation

```{r temporal_validation, eval=FALSE}
# Time-based validation for temporal data
temporal_validation <- function(data, time_col, validation_years = 2) {
  
  # Determine split point
  max_year <- max(data[[time_col]], na.rm = TRUE)
  split_year <- max_year - validation_years
  
  # Create temporal split
  temporal_split <- rsample::initial_time_split(
    data = data,
    prop = sum(data[[time_col]] <= split_year) / nrow(data)
  )
  
  return(temporal_split)
}
```

# Integration with External Tools

## Export Models for Production

```{r model_export, eval=FALSE}
# Export trained models for production use
export_production_model <- function(ensemble_fit, output_dir) {
  
  # Create output directory
  fs::dir_create(output_dir)
  
  # Export ensemble object
  saveRDS(ensemble_fit, file.path(output_dir, "ensemble_model.rds"))
  
  # Export preprocessing steps
  preprocessing_recipe <- ensemble_fit$recipe
  saveRDS(preprocessing_recipe, file.path(output_dir, "preprocessing_recipe.rds"))
  
  # Export model metadata
  metadata <- list(
    model_type = "stacked_ensemble",
    training_date = Sys.Date(),
    performance_metrics = ensemble_fit$performance_metrics,
    member_models = names(ensemble_fit$member_fits),
    feature_names = ensemble_fit$recipe$var_info$variable[
      ensemble_fit$recipe$var_info$role == "predictor"
    ]
  )
  
  jsonlite::write_json(metadata, file.path(output_dir, "model_metadata.json"))
  
  # Create prediction function
  predict_function <- function(new_data) {
    # Apply preprocessing
    processed_data <- recipes::bake(preprocessing_recipe, new_data)
    
    # Generate predictions
    predictions <- predict(ensemble_fit, processed_data)
    
    return(predictions)
  }
  
  # Save prediction function
  saveRDS(predict_function, file.path(output_dir, "predict_function.rds"))
  
  cli::cli_alert_success("Model exported to {output_dir}")
}
```

## Integration with MLflow

```{r mlflow_integration, eval=FALSE}
# Log experiments with MLflow
log_experiment <- function(model_results, experiment_name) {
  
  library(mlflow)
  
  # Start MLflow run
  mlflow_start_run(experiment_id = mlflow_create_experiment(experiment_name))
  
  # Log parameters
  mlflow_log_param("model_type", model_results$model_type)
  mlflow_log_param("preprocessing", model_results$preprocessing)
  mlflow_log_param("n_features", model_results$n_features)
  
  # Log metrics
  mlflow_log_metric("rrmse", model_results$rrmse)
  mlflow_log_metric("r_squared", model_results$r_squared)
  mlflow_log_metric("mae", model_results$mae)
  
  # Log model artifact
  mlflow_log_model(model_results$fitted_model, "model")
  
  # End run
  mlflow_end_run()
}
```

# Summary

This vignette demonstrated advanced modeling capabilities in `horizons`, including:

- Custom model specifications and hyperparameter tuning
- Detailed ensemble stacking and interpretation
- Performance optimization strategies
- Advanced preprocessing and validation techniques
- Production deployment considerations

For additional resources, see the other vignettes:
- **Preprocessing and Feature Selection**: Detailed preprocessing workflows
- **Covariate Integration**: Working with soil and climate data
- **Getting Started**: Basic workflow overview

# Session Information

```{r session_info}
sessionInfo()
```